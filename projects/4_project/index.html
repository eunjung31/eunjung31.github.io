<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Neural Networks | Eunjung Yeo </title> <meta name="author" content="Eunjung Yeo"> <meta name="description" content="Analysis on neural network representations and capabilities"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?08d14a9484dd54bd2a1749201853911b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://eunjung31.github.io/projects/4_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eunjung</span> Yeo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Neural Networks</h1> <p class="post-description">Analysis on neural network representations and capabilities</p> </header> <article> <p>Despite significant advances in speech technology, much remains unknown about what neural models truly learn and what they fail to capture. Understanding the representations and limitations of these models is crucial for designing robust, interpretable systems. Through our analysis, we aim to uncover how neural networks process information and how these insights can enhance their performance and fairness across diverse populations. My research specifically focuses on computational phonetics and phonology, exploring how these models encode fine-grained linguistic details and variability.</p> <p>In <a class="citation" href="#choi2022opening">(Choi* &amp; Yeo*, 2022)</a>, we analyzed what phoentic/phonemic information are encoded in wav2vec2.0 variants. We are on the way of follow-up studies, so please stay tuned!</p> <p>In <a class="citation" href="#huang2024dynamicsuperbphase2collaborativelyexpanding">(Huang et al., 2024)</a>, CMU-Linguistics team consist of <a href="https://kwangheechoi.com" rel="external nofollow noopener" target="_blank">Kwanghee Choi</a>, <a href="https://scholar.google.com/citations?user=AtEp3vUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Kalvin Chang</a>, and myself, has proposed various tasks related to phonetics, phonology, and prosody, focusing on the capabilities of speech foundation models in addressing linguistic theory-based tasks.</p> <p><a class="citation" href="#choi2025leveragingallophonyselfsupervisedspeech">(Choi et al., 2025)</a></p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NAACL main</abbr> </div> <div id="choi2025leveragingallophonyselfsupervisedspeech" class="col-sm-8"> <div class="title">Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment</div> <div class="author"> <a href="https://kwangheechoi.com" rel="external nofollow noopener" target="_blank">Kwanghee Choi</a>, <em>Eunjung Yeo</em>, <a href="https://scholar.google.com/citations?user=AtEp3vUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Kalvin Chang</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Shinji Watanabe, David Mortensen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="huang2024dynamicsuperbphase2collaborativelyexpanding" class="col-sm-8"> <div class="title">Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks</div> <div class="author"> Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, and <span class="more-authors" title="click to view 75 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '75 more authors' ? 'Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Fabian Ritter-Gutierrez, Ming To Chuang, Kuan-Po Huang, Siddhant Arora, You-Kuan Lin, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, Cheng-Hsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor R. Guimarães, Jionghao Han, Tzu-Quan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, Yu-Hua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, Kuan-Yu Fang Chiang, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, Hsi-Che Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Jui-Chiang Wei, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Vahid Noroozi, Zhehuai Chen, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, Hung-yi Lee' : '75 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">75 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="choi2022opening" class="col-sm-8"> <div class="title">Opening the black box of wav2vec feature encoder</div> <div class="author"> <a href="https://kwangheechoi.com" rel="external nofollow noopener" target="_blank">Kwanghee Choi<sup>*</sup></a>, and <em>Eunjung Yeo<sup>*</sup></em> </div> <div class="periodical"> <em>arXiv preprint arXiv:2210.15386</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eunjung Yeo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"* denotes equal contribution.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Ongoing Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-people",title:"people",description:"My Mentors and Collaborators!",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"news-started-my-new-academic-journey-as-a-visiting-scholar-at-language-technologies-institute-carnegie-mellon-university-sparkles",title:"Started my new academic journey as a Visiting Scholar at Language Technologies Institute,...",description:"",section:"News"},{id:"news-successfully-defended-my-ph-d-dissertation-mortar-board",title:'Successfully defended my Ph.D. Dissertation! <img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20">',description:"",section:"News"},{id:"news-incorporated-korean-g2p-in-epitran-with-my-dear-colleague-kwanghee-choi",title:"Incorporated Korean G2P in Epitran with my dear colleague, Kwanghee Choi.",description:"",section:"News"},{id:"news-contributed-to-phonetics-phonology-and-prosody-tasks-in-dynamic-superb-phase-2-as-part-of-the-cmu-linguistics-team-with-kwanghee-choi-kalvin-chang-shinji-watanabe-and-david-mortensen",title:"Contributed to Phonetics, Phonology, and Prosody tasks in Dynamic-SUPERB Phase-2, as part of...",description:"",section:"News"},{id:"news-presented-three-papers-at-sane-speech-and-audio-in-the-northeast-workshop-held-in-boston-1-2-mouse2",title:"Presented three papers at SANE (Speech and Audio in the Northeast) workshop, held...",description:"",section:"News"},{id:"news-dynamic-superb-phase-2-has-been-accepted-to-iclr-2025-dart",title:'Dynamic-SUPERB Phase-2 has been accepted to ICLR 2025! <img class="emoji" title=":dart:" alt=":dart:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png" height="20" width="20">',description:"",section:"News"},{id:"news-our-paper-has-been-accepted-to-naacl-main-tada",title:'Our paper has been accepted to NAACL (main)! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">',description:"",section:"News"},{id:"news-two-of-our-papers-have-been-accepted-to-interspeech-2025-1-2",title:"Two of our papers have been accepted to Interspeech 2025! [1][2]\ud83c\udf1f",description:"",section:"News"},{id:"news-i-have-started-my-new-academic-journey-as-a-visiting-scholar-in-the-department-of-computer-science-at-the-university-of-texas-at-austin",title:"I have started my new academic journey as a Visiting Scholar in the...",description:"",section:"News"},{id:"news-our-paper-on-cross-language-intelligibility-assessment-leveraging-ai-has-been-accepted-in-perspectives-of-the-asha-sig-19",title:"Our paper on cross-language intelligibility assessment leveraging AI has been accepted in Perspectives...",description:"",section:"News"},{id:"projects-cross-language-intelligibility-assessment",title:"Cross-Language Intelligibility Assessment",description:"Intelligibility assessment applicable across various languages",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-acoustic-analysis-of-pathological-speech",title:"Acoustic Analysis of Pathological Speech",description:"Automatic analysis of pathological speech considering various speech dimensions",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-inclusive-asr",title:"Inclusive ASR",description:"Develop inclusive technology for pathological speech",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-understanding-neural-networks",title:"Understanding Neural Networks",description:"Analysis on neural network representations and capabilities",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-universal-phone-recognition",title:"Universal Phone Recognition",description:"Recognizing phonetic units in a language-neural fashion",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-pronunciation-analysis-of-l2-speech",title:"Pronunciation Analysis of L2 Speech",description:"Objective analysis of L2 speech pronunciation",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%65%79%65%6F%32@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=hSSNI4EAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/eunjung31","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/eunjung-yeo-078776194","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>